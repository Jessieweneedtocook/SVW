{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jessieweneedtocook/SPV.git"
      ],
      "metadata": {
        "id": "FQDTXoG5PNhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd SPV"
      ],
      "metadata": {
        "id": "iYXqa1fePQ3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "metadata": {
        "id": "TgNSd6LJPUsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets diffusers transformers accelerate"
      ],
      "metadata": {
        "id": "QKBEi57dbyo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import timm\n",
        "class StabilityPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = timm.create_model(\"convnext_tiny\", pretrained=True, features_only=True)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(768, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(256, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)[-1]              # [B, 96, 8, 8]\n",
        "        x = self.decoder(x)                  # [B, 1, 16, 16]\n",
        "        x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hXX5q4yUUcyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9CgFMY-Bj9dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = StabilityPredictor().to(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/stability_predictor.pth\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "DA-DNe9-PXan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vine.src.vine_turbo import VINE_Turbo, VAE_encode, VAE_decode, initialize_unet_no_lora, initialize_vae_no_lora\n",
        "from vine.src.stega_encoder_decoder import ConditionAdaptor, CustomConvNeXt\n",
        "from vine.src.model import make_1step_sched\n",
        "from accelerate.utils import set_seed\n",
        "import torch, os, gc\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# for p in model.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "\n",
        "watermarker = VINE_Turbo(\n",
        "    ckpt_path=None,\n",
        "    device='cuda',\n",
        "    stability_predictor=model,\n",
        "    tensor_six=False\n",
        ")\n",
        "watermarker.load_state_dict(torch.load(\"/content/drive/MyDrive/wm_finetuned2_epochRB2.pth\"))\n",
        "watermarker.to(device)\n",
        "watermarker.train()\n",
        "for p in watermarker.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "watermarker.stability_predictor.eval()\n",
        "for p in watermarker.stability_predictor.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "decoder = CustomConvNeXt(secret_size=100).to(device)\n",
        "decoder.convnext.classifier = nn.Sequential(\n",
        "    nn.Flatten(1),\n",
        "    nn.Linear(1024, 100, bias=True)\n",
        ")\n",
        "decoder.load_state_dict(torch.load(\"/content/drive/MyDrive/decoder_finetuned2_epochRB2.pth\"))\n",
        "decoder.train()\n",
        "for p in decoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "params = list(watermarker.parameters()) + list(decoder.parameters())\n",
        "\n",
        "print(\"\\n✅ Stage 1: ConditionAdaptor and Decoder will be trained. VAE, UNet, StabilityPredictor are frozen.\")\n"
      ],
      "metadata": {
        "id": "nUhzGbraAYjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vine.src.vine_turbo import VINE_Turbo, VAE_encode, VAE_decode, initialize_unet_no_lora, initialize_vae_no_lora\n",
        "from vine.src.stega_encoder_decoder import ConditionAdaptor, CustomConvNeXt\n",
        "from vine.src.model import make_1step_sched\n",
        "from accelerate.utils import set_seed\n",
        "import torch, os, gc\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# for p in model.parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "\n",
        "# watermarker = VINE_Turbo.from_pretrained('Shilin-LU/VINE-B-Enc',\n",
        "#     stability_predictor=model,  # your pretrained stability model\n",
        "#     tensor_six=True  # unless you're using PretrainedConditionAdaptor\n",
        "# )\n",
        "# watermarker.load_state_dict(torch.load(\"/content/drive/MyDrive/wm_finetuned_epochF.pth\"))\n",
        "\n",
        "watermarker = VINE_Turbo(\n",
        "    ckpt_path=None,\n",
        "    device='cuda',\n",
        "    stability_predictor=model,\n",
        "    tensor_six=False\n",
        ")\n",
        "watermarker.to(device)\n",
        "watermarker.eval()\n",
        "for p in watermarker.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "watermarker.sec_encoder = ConditionAdaptor().to(device)\n",
        "for p in watermarker.sec_encoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "watermarker.vae_a2b.encoder.train()\n",
        "for p in watermarker.vae_a2b.encoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "decoder = CustomConvNeXt(secret_size=100).to(device)\n",
        "decoder.convnext.classifier = nn.Sequential(\n",
        "    nn.Flatten(1),\n",
        "    nn.Linear(1024, 100, bias=True)\n",
        ")\n",
        "decoder.train()\n",
        "for p in decoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "params = list(decoder.parameters()) + list(watermarker.sec_encoder.parameters()) + list(watermarker.vae_a2b.encoder.parameters())\n",
        "print(\"\\n✅ Stage 1: ConditionAdaptor and Decoder will be trained. VAE, UNet, StabilityPredictor are frozen.\")"
      ],
      "metadata": {
        "id": "cL0oXcGcep2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm opencv-python matplotlib Pillow\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "uvERdqutKa_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import timm"
      ],
      "metadata": {
        "id": "iqfazncnUKmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_img(img, index, convert_mask=False):\n",
        "    resize_size = (512, 512)\n",
        "    img = img.convert(\"RGB\").resize(resize_size, Image.BILINEAR)\n",
        "    img_path = f\"data/img_{index:05d}.jpg\"\n",
        "\n",
        "    img.save(img_path)"
      ],
      "metadata": {
        "id": "60EevJO5UM9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset_stream = load_dataset(\"paint-by-inpaint/PIPE\", split=\"train\", streaming=True)\n",
        "dataset = list(islice(dataset_stream, 5000))"
      ],
      "metadata": {
        "id": "Fz_udu2qltrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/data\", exist_ok=True)\n",
        "for i, entry in enumerate(tqdm(dataset)):\n",
        "    save_img(entry[\"target_img\"], i)"
      ],
      "metadata": {
        "id": "JMcFFtzWUQ-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del dataset"
      ],
      "metadata": {
        "id": "f79yeBDGUTH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_dataset_stream = load_dataset(\"paint-by-inpaint/PIPE\", split=\"train\", streaming=True)\n",
        "pipe_dataset = list(islice(pipe_dataset_stream, 5000, 10000))"
      ],
      "metadata": {
        "id": "taVJBuq9q2Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = len(os.listdir(\"/content/data\"))\n",
        "for i, entry in enumerate(tqdm(pipe_dataset)):\n",
        "    save_img(entry[\"target_img\"], counter + i)"
      ],
      "metadata": {
        "id": "oZYWFcxOZqrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe_dataset"
      ],
      "metadata": {
        "id": "g2Q8a332UWze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "\n",
        "dataset_stream = load_dataset(\"timbrooks/instructpix2pix-clip-filtered\", split=\"train\", streaming=True)\n",
        "\n",
        "os.makedirs(\"/content/data/images/class0\", exist_ok=True)\n",
        "\n",
        "csv_path = \"/content/data/edit_prompts.csv\"\n",
        "records = []\n",
        "\n",
        "for i, item in enumerate(tqdm(islice(dataset_stream, 10000))):\n",
        "    image = item[\"original_image\"]\n",
        "    prompt = item[\"edit_prompt\"]\n",
        "    filename = f\"img_{i:05d}.jpg\"\n",
        "    image.save(f\"/content/data/images/class0/{filename}\")\n",
        "    records.append({\"filename\": filename, \"edit_prompt\": prompt})\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "del dataset_stream"
      ],
      "metadata": {
        "id": "XWAQedX5H7tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data/images/class0\n",
        "!find /content/data -maxdepth 1 -iname \"*.jpg\" -exec mv {} /content/data/images/class0/ \\;"
      ],
      "metadata": {
        "id": "0v4F4LNUU-hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "epochs = 3\n",
        "batch_size = 8\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root='/content/data/images', transform=transform)\n",
        "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "OEp8T3VhCfxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "secret_dim = 100\n",
        "def generate_random_watermark(batch_size):\n",
        "    return torch.rand(batch_size, secret_dim).to(device) * 0.9 + 0.05\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5)"
      ],
      "metadata": {
        "id": "mZI3Z7aAErYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "distortion_strength_paras = {\n",
        "    'brightness':   (0.9, 2.5),\n",
        "    'contrast':     (0.9, 2.5),\n",
        "    'saturation':   (0.9, 2.5),\n",
        "    'blurring':     (0.0, 15.0),\n",
        "    'motion_blur':  (1, 10),\n",
        "    'zoom_blur':    (1.0, 4.0),\n",
        "    'pixelation':   (1, 20),\n",
        "    'noise':        (0.0, 2)\n",
        "}\n",
        "# #2\n",
        "# distortion_strength_paras = {\n",
        "#     'brightness':   (0.3, 1.3),\n",
        "#     'contrast':     (0.3, 1.3),\n",
        "#     'saturation':   (0.3, 1.3),\n",
        "#     'blurring':     (0.0, 5.0),\n",
        "#     'motion_blur':  (1, 6),\n",
        "#     'zoom_blur':    (1.0, 2.0),\n",
        "#     'pixelation':   (1, 8),\n",
        "#     'noise':        (0.0, 0.1),\n",
        "#     'compression':  (100, 30)\n",
        "# }\n",
        "# #3\n",
        "# distortion_strength_paras = {\n",
        "#     'brightness':   (0.3, 1.3),\n",
        "#     'contrast':     (0.3, 1.3),\n",
        "#     'saturation':   (0.3, 1.3),\n",
        "#     'blurring':     (0.0, 5.0),\n",
        "#     'motion_blur':  (1, 6),\n",
        "#     'zoom_blur':    (1.0, 2.0),\n",
        "#     'pixelation':   (1, 8),\n",
        "#     'noise':        (0.0, 0.1),\n",
        "#     'compression':  (100, 30)\n",
        "# }"
      ],
      "metadata": {
        "id": "sPmhCq2QyVgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def apply_single_distortion(image, distortion_type, strength=None):\n",
        "    assert distortion_type in distortion_strength_paras.keys(), f\"Unsupported distortion: {distortion_type}\"\n",
        "\n",
        "    if distortion_type == \"brightness\":\n",
        "        image = image * strength\n",
        "        image = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "    elif distortion_type == \"contrast\":\n",
        "        mean = image.mean(dim=(1, 2), keepdim=True)\n",
        "        image = (image - mean) * strength + mean\n",
        "        image = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "    elif distortion_type == \"saturation\":\n",
        "        gray = image.mean(dim=0, keepdim=True)\n",
        "        image = (image - gray) * strength + gray\n",
        "        image = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "    elif distortion_type == \"blurring\":\n",
        "        if strength > 0:\n",
        "            kernel_size = int(strength * 2) | 1\n",
        "            image = TF.gaussian_blur(image, kernel_size=kernel_size, sigma=strength)\n",
        "\n",
        "    elif distortion_type == \"motion_blur\":\n",
        "        if strength > 0:\n",
        "            kernel_size = int(strength) | 1\n",
        "            image = TF.gaussian_blur(image, kernel_size=kernel_size, sigma=strength/2)\n",
        "\n",
        "    elif distortion_type == \"zoom_blur\":\n",
        "        zoom_factor = strength\n",
        "        C, H, W = image.shape\n",
        "        zoomed = F.interpolate(image.unsqueeze(0), scale_factor=zoom_factor, mode='bilinear', align_corners=False)[0]\n",
        "        zoomed = TF.center_crop(zoomed, (H, W))\n",
        "        image = 0.5 * image + 0.5 * zoomed\n",
        "\n",
        "    elif distortion_type == \"pixelation\":\n",
        "        factor = max(1, int(strength))\n",
        "        C, H, W = image.shape\n",
        "        small = F.interpolate(image.unsqueeze(0), size=(H//factor, W//factor), mode='nearest')\n",
        "        image = F.interpolate(small, size=(H, W), mode='nearest')[0]\n",
        "\n",
        "    elif distortion_type == \"noise\":\n",
        "        noise = torch.randn_like(image) * strength\n",
        "        image = image + noise\n",
        "        image = torch.clamp(image, 0.0, 1.0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported distortion: {distortion_type}\")\n",
        "\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "wzFkC9fcfdED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch\n",
        "\n",
        "def random_distort_batch(batch):\n",
        "    B, C, H, W = batch.shape\n",
        "    out = []\n",
        "\n",
        "    # fig, axs = plt.subplots(1, B, figsize=(4*B, 4))\n",
        "\n",
        "    for i in range(B):\n",
        "        img = batch[i]  # Tensor already on GPU, [0, 1]\n",
        "\n",
        "        # Pick one random distortion\n",
        "        op = random.choice(list(distortion_strength_paras.keys()))\n",
        "        min_val, max_val = distortion_strength_paras[op]\n",
        "        strength = random.uniform(min_val, max_val)\n",
        "\n",
        "        # Apply distortion\n",
        "        img = apply_single_distortion(img, op, strength)\n",
        "\n",
        "        img = torch.clamp(img, 0.0, 1.0)\n",
        "\n",
        "        # Visualize\n",
        "        # img_pil = TF.to_pil_image(img.cpu())\n",
        "\n",
        "        # ax = axs[i] if B > 1 else axs\n",
        "        # ax.imshow(img_pil)\n",
        "        # ax.axis('off')\n",
        "        # ax.set_title(f\"{op}\\n{strength:.2f}\")\n",
        "\n",
        "        out.append(img)\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    return torch.stack(out)\n"
      ],
      "metadata": {
        "id": "orbTkhce-XtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import torchvision.transforms.functional as TF\n",
        "import io\n",
        "import numpy as np\n",
        "from vine.w_bench_utils.distortion.utils import to_tensor, to_pil\n",
        "\n",
        "def apply_single_distortion(image, distortion_type, strength=None):\n",
        "    assert distortion_type in distortion_strength_paras.keys(), f\"Unsupported distortion: {distortion_type}\"\n",
        "\n",
        "    if distortion_type == \"brightness\":\n",
        "        enhancer = ImageEnhance.Brightness(image)\n",
        "        image = enhancer.enhance(strength)\n",
        "\n",
        "    elif distortion_type == \"contrast\":\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(strength)\n",
        "\n",
        "    elif distortion_type == \"saturation\":\n",
        "        enhancer = ImageEnhance.Color(image)\n",
        "        image = enhancer.enhance(strength)\n",
        "\n",
        "    elif distortion_type == \"blurring\":\n",
        "        image = image.filter(ImageFilter.GaussianBlur(radius=strength))\n",
        "\n",
        "    elif distortion_type == \"motion_blur\":\n",
        "        k = int(strength)\n",
        "        k = k + 1 if k % 2 == 0 else k\n",
        "        image = image.filter(ImageFilter.BoxBlur(k // 2))\n",
        "\n",
        "    elif distortion_type == \"zoom_blur\":\n",
        "        zoom = strength\n",
        "        w, h = image.size\n",
        "        zoomed = image.resize((int(w * zoom), int(h * zoom)), Image.BILINEAR)\n",
        "        left = (zoomed.width - w) // 2\n",
        "        top = (zoomed.height - h) // 2\n",
        "        zoomed = zoomed.crop((left, top, left + w, top + h))\n",
        "        image = Image.blend(image, zoomed, alpha=0.5)\n",
        "\n",
        "    elif distortion_type == \"pixelation\":\n",
        "        factor = max(1, int(strength))\n",
        "        small = image.resize((image.width // factor, image.height // factor), Image.NEAREST)\n",
        "        image = small.resize(image.size, Image.NEAREST)\n",
        "\n",
        "    elif distortion_type == \"noise\":\n",
        "        tensor = to_tensor([image], norm_type=None)\n",
        "        noise = torch.randn(tensor.size()) * strength\n",
        "        image = to_pil((tensor + noise).clamp(0, 1), norm_type=None)[0]\n",
        "\n",
        "    elif distortion_type == \"compression\":\n",
        "        buffer = io.BytesIO()\n",
        "        image.save(buffer, format=\"JPEG\", quality=int(strength))\n",
        "        image = Image.open(buffer)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported distortion: {distortion_type}\")\n",
        "\n",
        "    return image.convert(\"RGB\")"
      ],
      "metadata": {
        "id": "FN-iepSB2XHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_distort_batch(batch):\n",
        "    B, C, H, W = batch.shape\n",
        "    out = []\n",
        "\n",
        "    # fig, axs = plt.subplots(1, B, figsize=(4*B, 4))\n",
        "\n",
        "    for i in range(B):\n",
        "        img = TF.to_pil_image(batch[i].cpu())\n",
        "\n",
        "        op = random.choice(list(distortion_strength_paras.keys()))\n",
        "        min_val, max_val = distortion_strength_paras[op]\n",
        "        strength = random.uniform(min_val, max_val)\n",
        "\n",
        "\n",
        "        img = apply_single_distortion(img, op, strength)\n",
        "\n",
        "        # ax = axs[i] if B > 1 else axs\n",
        "        # ax.imshow(img)\n",
        "        # ax.axis('off')\n",
        "        # ax.set_title(f\"{op}\\n{strength:.2f}\")\n",
        "\n",
        "\n",
        "        out.append(TF.to_tensor(img))\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    return torch.stack(out)"
      ],
      "metadata": {
        "id": "4RIKnd4H2bjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "prompt_df = pd.read_csv(\"/content/data/edit_prompts.csv\")\n",
        "\n",
        "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
        "    \"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "pipe.safety_checker = None\n",
        "\n",
        "to_pil = lambda x: TF.to_pil_image(x.cpu())\n",
        "to_tensor = lambda x: TF.to_tensor(x).to(\"cuda\")\n",
        "\n",
        "def maybe_edit_batch(batch, batch_idx):\n",
        "    edited_batch = []\n",
        "    batch_size = batch.size(0)\n",
        "\n",
        "    for i, img_tensor in enumerate(batch):\n",
        "        img_idx = batch_idx * batch_size + i\n",
        "\n",
        "        if img_idx >= len(prompt_df):\n",
        "            edited_batch.append(img_tensor)\n",
        "            continue\n",
        "\n",
        "        prompt = prompt_df.iloc[img_idx][\"edit_prompt\"]\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            pil_img = to_pil(img_tensor)\n",
        "            guidance = random.uniform(7.0, 9.0)\n",
        "\n",
        "            try:\n",
        "                edited = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=pil_img,\n",
        "                    num_inference_steps=20,\n",
        "                    image_guidance_scale=1.5,\n",
        "                    guidance_scale=guidance\n",
        "                ).images[0]\n",
        "                edited_batch.append(to_tensor(edited))\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Edit failed on idx {img_idx}: {e}\")\n",
        "                edited_batch.append(img_tensor)\n",
        "        else:\n",
        "            edited_batch.append(img_tensor)\n",
        "\n",
        "    return torch.stack(edited_batch)\n"
      ],
      "metadata": {
        "id": "DnFsu2AM1bRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "zHLAO9f_4vD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "id": "3vPMD58MQS4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lpips\n",
        "lpips_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "lpips_loss_fn.eval()\n",
        "lpips_loss_fn.requires_grad_(False)"
      ],
      "metadata": {
        "id": "QH73BXBHQ8uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bit_accuracy(pred_logits, target_bits):\n",
        "    preds = (torch.sigmoid(pred_logits) > 0.5).float()\n",
        "    correct_bits = (preds == target_bits).float().sum()\n",
        "    total_bits = torch.numel(preds)\n",
        "    return (correct_bits / total_bits).item()"
      ],
      "metadata": {
        "id": "5EgShxcSB61d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import AutoTokenizer, CLIPTextModel\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "# for model in [decoder, vae, unet, condition_adaptor, model]:\n",
        "#     model.to(device)\n",
        "for models in [decoder, watermarker]:\n",
        "    models.to(device)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = total_secret_loss = total_recon_loss = total_embed_mask_loss = total_lpips_loss = total_bit_acc = 0\n",
        "    # if epoch == 4:\n",
        "    #     for p in decoder.parameters():\n",
        "    #         p.requires_grad = True\n",
        "    #     params = list(watermarker.parameters()) + list(decoder.parameters())\n",
        "    #     optimizer = torch.optim.AdamW(params, lr=1e-5)\n",
        "    batch_ind = 0\n",
        "    for imgs, _ in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        imgs = imgs.to(device)\n",
        "        B = imgs.size(0)\n",
        "\n",
        "        watermark = torch.randint(0, 2, (B, 100), device=device).float()\n",
        "\n",
        "        resized_imgs = F.interpolate(imgs, size=(256, 256), mode='bicubic', align_corners=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            stability_mask = model(resized_imgs)  # shape: [B, 1, H, W]\n",
        "\n",
        "        resized_img = resized_imgs * 2.0 - 1.0\n",
        "\n",
        "        input_image = imgs\n",
        "\n",
        "        input_image = 2.0 * input_image - 1.0\n",
        "\n",
        "\n",
        "        mask = torch.nan_to_num(stability_mask.clone().float(), nan=0.0, posinf=1.0, neginf=0.0)\n",
        "        B, _, H, W = mask.shape\n",
        "        flat_mask = mask.view(B, -1)\n",
        "        k = (H * W) // 2\n",
        "        topk = torch.topk(flat_mask, k=k, largest=True, dim=1).indices\n",
        "        binary_mask = torch.zeros_like(flat_mask)\n",
        "        binary_mask.scatter_(1, topk, 1.0)\n",
        "        mask = binary_mask.view(B, 1, H, W)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "\n",
        "            x_out_decoded = watermarker(resized_img, secret=watermark)\n",
        "\n",
        "            x_out_decoded = torch.nan_to_num(x_out_decoded, nan=0.0, posinf=1.0, neginf=0.0).clamp(-1.0, 1.0)\n",
        "\n",
        "            residual_256 = x_out_decoded - resized_img\n",
        "\n",
        "            residual_512 = F.interpolate(residual_256, size=(512, 512), mode='bicubic', align_corners=False)\n",
        "\n",
        "            encoded_image = residual_512 + input_image\n",
        "\n",
        "            encoded_image = encoded_image * 0.5 + 0.5\n",
        "            encoded_image = torch.clamp(encoded_image, min=0.0, max=1.0)\n",
        "\n",
        "            # x_out_decoded = torch.nan_to_num(x_out_decoded, nan=0.0, posinf=1.0, neginf=0.0).clamp(-1.0, 1.0)\n",
        "            # residual = x_out_decoded - imgs  # imgs is already [-1, 1]\n",
        "            # residual_upsampled = F.interpolate(residual, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
        "            # imgs_upsampled = F.interpolate(imgs, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
        "            # reconstructed_img = imgs_upsampled + residual_upsampled\n",
        "            # reconstructed_img = reconstructed_img.clamp(-1.0, 1.0)\n",
        "\n",
        "            # Now match decoder preprocessing exactly\n",
        "            # x_out_resized = F.interpolate(reconstructed_img, size=(256, 256), mode=\"bicubic\", align_corners=False)\n",
        "            # x_out_resized = (x_out_resized + 1.0) / 2.0\n",
        "            if random.random() < 0.9:\n",
        "                encoded_image_distort = random_distort_batch(encoded_image).to(device)\n",
        "            else:\n",
        "                encoded_image_distort = maybe_edit_batch(encoded_image, batch_ind).to(device)\n",
        "\n",
        "            # x_out_resized = maybe_edit_batch(x_out_resized, batch_ind).to(device)\n",
        "\n",
        "\n",
        "            encoded_image_256 = F.interpolate(encoded_image_distort, size=(256, 256), mode='bicubic', align_corners=False)\n",
        "\n",
        "            pred_watermark = decoder(encoded_image_256)\n",
        "\n",
        "            bit_acc = bit_accuracy(pred_watermark.detach(), watermark)\n",
        "\n",
        "            x_sec = watermarker.sec_encoder(watermark, resized_img, stability_mask)\n",
        "\n",
        "            delta = torch.abs(x_sec - resized_img)\n",
        "            embed_mask_loss = torch.mean(delta * mask)\n",
        "\n",
        "            x_out_lpips = (encoded_image + 1.0) / 2.0\n",
        "            imgs_lpips = (imgs + 1.0) / 2.0\n",
        "            lpips_loss = lpips_loss_fn(x_out_lpips, imgs_lpips).mean()\n",
        "\n",
        "            recon_loss = F.mse_loss(encoded_image, imgs)\n",
        "            secret_loss = F.binary_cross_entropy_with_logits(pred_watermark, watermark)\n",
        "\n",
        "            loss = secret_loss * 2 + embed_mask_loss * 1.5 + recon_loss * 2  + lpips_loss * 1.5\n",
        "            # + 2.0 * embed_mask_loss  ← optional if you want to penalize editing stable regions\n",
        "\n",
        "            total_bit_acc += bit_acc\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"NaN detected, skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        for model_to_clip in [decoder, watermarker]:\n",
        "            torch.nn.utils.clip_grad_norm_(model_to_clip.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_secret_loss += secret_loss.item()\n",
        "        total_recon_loss += recon_loss.item()\n",
        "        total_embed_mask_loss += embed_mask_loss.item()\n",
        "        total_lpips_loss += lpips_loss.item()\n",
        "\n",
        "        batch_ind += 1\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    avg_secret = total_secret_loss / len(loader)\n",
        "    avg_recon = total_recon_loss / len(loader)\n",
        "    avg_lpips = total_lpips_loss / len(loader)\n",
        "    avg_embed = total_embed_mask_loss / len(loader)\n",
        "    avg_bit_acc = total_bit_acc / len(loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {avg_loss:.4f} | Secret: {avg_secret:.4f} | EmbedMask: {avg_embed:.4f} | Recon: {avg_recon:.4f} | LPIPS: {avg_lpips:.4f} | BitAcc: {avg_bit_acc:.4f}\")\n",
        "\n",
        "    #torch.save(watermarker.vae_a2b.encoder.state_dict(), f\"/content/drive/MyDrive/vae_enc_finetuned_epoch{epoch+1}.pth\")\n",
        "    #torch.save(watermarker.sec_encoder.state_dict(), f\"/content/drive/MyDrive/condition_adaptor_finetuned_epoch{epoch+1}.pth\")\n",
        "\n",
        "torch.save(decoder.state_dict(), f\"/content/drive/MyDrive/decoder_finetuned2_epoch{epoch+1}.pth\")\n",
        "torch.save(watermarker.state_dict(), f\"/content/drive/MyDrive/wm_finetuned2_epoch{epoch+1}.pth\")"
      ],
      "metadata": {
        "id": "daWgCc_yJuAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "image_path = \"/content/data/images/class0\"\n",
        "image_files = sorted([f for f in os.listdir(image_path) if f.lower().endswith((\".jpg\", \".png\"))])\n",
        "img = image_files[567]\n",
        "img = Image.open(os.path.join(image_path, img)).convert(\"RGB\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "img_tensor = transform(img).unsqueeze(0).to(device)  # shape: [1, 3, 256, 256]\n",
        "\n",
        "\n",
        "watermark = generate_random_watermark(1)  # shape: [1, 100]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    stability_mask = model(img_tensor)  # shape: [1, 1, 256, 256]\n",
        "\n",
        "\n",
        "flat_mask = stability_mask.view(1, -1)\n",
        "k = (256 * 256) // 2\n",
        "_, topk_indices = torch.topk(flat_mask, k=k, largest=True, dim=1)\n",
        "binary_mask = torch.zeros_like(flat_mask)\n",
        "binary_mask.scatter_(1, topk_indices, 1.0)\n",
        "mask = binary_mask.view(1, 1, 256, 256)  # 1 = stable\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_sec = watermarker.sec_encoder(watermark, img_tensor, stability_mask)\n",
        "\n",
        "\n",
        "def visualize_watermark_embedding(img, x_sec, mask):\n",
        "    img = img.squeeze().detach().cpu()\n",
        "    x_sec = x_sec.squeeze().detach().cpu()\n",
        "    delta = torch.abs(img - x_sec).mean(dim=0)\n",
        "    stability_mask = 1 - mask\n",
        "    mask = stability_mask.squeeze().detach().cpu()\n",
        "\n",
        "    plt.figure(figsize=(15, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(delta, cmap='gnuplot')\n",
        "    plt.title(\"Embedding Heatmap (Δ from x_sec)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Stability Mask (1 = Stable)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_watermark_embedding(img_tensor, x_sec, mask)\n"
      ],
      "metadata": {
        "id": "S76gUZgN4izh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    x_out = watermarker(img_tensor, secret=watermark)\n",
        "delta = torch.abs(img_tensor - x_out).mean(dim=1, keepdim=True)  # [1, 1, H, W]\n",
        "def visualize_embedding_map(original, watermarked, mask):\n",
        "    original = original.squeeze().detach().cpu()\n",
        "    watermarked = watermarked.squeeze().detach().cpu()\n",
        "    delta = torch.abs(original - watermarked).mean(dim=0)  # [H, W]\n",
        "    mask = mask.squeeze().detach().cpu()\n",
        "\n",
        "    plt.figure(figsize=(15, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original.permute(1, 2, 0).clamp(0, 1))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(delta, cmap=\"hot\")\n",
        "    plt.title(\"Watermark Δ Heatmap\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Stability Mask (1 = Stable)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "visualize_embedding_map(img_tensor, x_out, mask)"
      ],
      "metadata": {
        "id": "2vE3SkVUI3Cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}